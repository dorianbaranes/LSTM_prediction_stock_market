import os
import math
import numpy as np
import datetime as dt
from numpy import newaxis


import numpy as np




# Needed for reproducible results
np.random.seed(1)


class Model:
    def __init__(self):
        self.model = None
        self.neurons_per_lstm_layer = None
        self.lstm_layers = None
        self.input_timesteps = None
        self.input_dim = None

    def load_model(self, filepath):
        from keras.models import Sequential, load_model
        import keras


        keras.backend.clear_session()
        print("[Model] Loading model from file %s" % filepath)
        self.model = load_model(filepath)


    def build_model(self, configs, neurons_lstm, layer_lstm, sequence_length):
        from keras.layers import Dense, Activation, Dropout, LSTM
        from keras.models import Sequential, load_model

        import tensorflow as tf
        import keras

        # Needed for reproducible results
        np.random.seed(1)
        tf.random.set_seed(1)
        #Resets all state generated by Keras.
        keras.backend.clear_session()


        self.model = Sequential()
        self.neurons_per_lstm_layer = neurons_lstm
        self.lstm_layers = layer_lstm
        self.input_timesteps = sequence_length-1
        self.input_dim = len(configs['data']['columns'])

        for layer in configs["model"]["layers"]:
            neurons = layer["neurons"] if "neurons" in layer else None
            dropout_rate = layer["rate"] if "rate" in layer else None
            activation = layer["activation"] if "activation" in layer else None


            if layer["type"] == "dense":
                self.model.add(Dense(neurons, activation=activation))

            if layer["type"] == "dropout":
                self.model.add(Dropout(dropout_rate))

            if layer["type"] == "lstm_layers":
                for i in range(layer_lstm):
                    input_shape = (self.input_timesteps, self.input_dim) if i==0 else (None,None)
                    return_sequences = False if layer_lstm-1==i else True

                    self.model.add(
                        LSTM(
                            neurons_lstm,
                            input_shape=input_shape,
                            return_sequences=return_sequences,
                        )
                    )
                    self.model.add(Dropout(layer["dropout_rate"]))

        self.model.compile(
            loss=configs["model"]["loss"], optimizer=configs["model"]["optimizer"]
        )

        print("[Model] Model Compiled")


    def train_generator(
        self, data_train_gen, data_val_gen, epochs, batch_size, steps_per_epoch, save_dir, file_name_prefix
    ):

        print("[Model] Training Started")
        print(
            "[Model] %s epochs, %s batch size, %s batches per epoch %s lstm layers %s neurons per layer"
            % (
                epochs,
                batch_size,
                steps_per_epoch,
                self.lstm_layers,
                self.neurons_per_lstm_layer,
            )
        )
        save_fname_weight = os.path.join(save_dir, file_name_prefix + "_weight.h5")
        save_fname_model = os.path.join(save_dir, file_name_prefix + "_model.h5")

        callbacks = [
            # ModelCheckpoint(filepath=save_fname_weight, monitor='loss', save_best_only=True)
        ]

        if epochs>1: #run fit without validation
            self.model.fit_generator(
                data_train_gen,
                steps_per_epoch=steps_per_epoch,
                epochs=epochs-1,
                callbacks=callbacks,
                workers=os.cpu_count(),
                use_multiprocessing=True
            )

        history = self.model.fit_generator(
            data_train_gen,
            steps_per_epoch=steps_per_epoch,
            epochs=1,
            callbacks=callbacks,
            workers=os.cpu_count(),
            use_multiprocessing=True,
            validation_data=data_val_gen
        )
        self.model.save(save_fname_model)
        print("[Model] Training Completed. Model saved as %s" % save_fname_weight)
        train_loss = history.history['loss'][-1]
        val_loss = history.history['val_loss'][-1]
        return train_loss,val_loss

    def predict_point_by_point(self, data):
        # Predict each timestep given the last sequence of true data, in effect only predicting 1 step ahead each time
        print("[Model] Predicting Point-by-Point...")
        predicted = self.model.predict(data)
        predicted = np.reshape(predicted, (predicted.size,))
        return predicted

    def predict_sequences_multiple(self, data, window_size, prediction_len):
        # Predict sequence of 50 steps before shifting prediction run forward by 50 steps
        print("[Model] Predicting Sequences Multiple...")
        prediction_seqs = []
        len_data=len(data)
        for i in range(int( len_data/ prediction_len) + 1):
            if i * prediction_len>=len_data:
                break
            curr_frame = data[i * prediction_len]
            predicted = []
            for j in range(prediction_len):
                predicted.append(self.model.predict(curr_frame[newaxis, :, :], verbose=0)[0, 0])
                curr_frame = curr_frame[1:]
                curr_frame = np.insert(
                    curr_frame, [window_size - 2], predicted[-1], axis=0
                )
            prediction_seqs.append(predicted)
        return prediction_seqs

    def predict_sequence_full(self, data, window_size):
        # Shift the window by 1 new prediction each time, re-run predictions on new window
        print("[Model] Predicting Sequences Full...")
        curr_frame = data[0]
        predicted = []
        for i in range(len(data)):
            predicted.append(self.model.predict(curr_frame[newaxis, :, :])[0, 0])
            curr_frame = curr_frame[1:]
            curr_frame = np.insert(curr_frame, [window_size - 2], predicted[-1], axis=0)
        return predicted
